{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084bf465-20a0-4e9f-ba13-577feffc0385",
   "metadata": {},
   "source": [
    "### Modified example from https://github.com/turboderp/exllamav2/blob/master/examples/inference.py - simple text input->text output stored in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed21215b-99de-47ff-9e5b-d31484e3ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16b218c-6079-4741-8bc1-7ee916bea108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    "    ExLlamaV2Cache_Q4,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2BaseGenerator,\n",
    "    ExLlamaV2Sampler\n",
    ")\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a508cb-d399-4c12-9e19-7f8a4f9f5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and cache\n",
    "\n",
    "model_directory =  \"/shared/analyst/models/Meta-Llama-3-70B-Instruct-4.0bpw-h6-exl2\"#\"/shared/analyst/models/Llama-3-8B-Instruct-262k-5.0bpw-h6-exl2\"\n",
    "\n",
    "config = ExLlamaV2Config(model_directory)\n",
    "config.prepare()\n",
    "config.max_seq_len = 32000\n",
    "model = ExLlamaV2(config)\n",
    "cache = ExLlamaV2Cache_Q4(model, lazy = True, max_seq_len=config.max_seq_len)\n",
    "model.load_autosplit(cache)\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "\n",
    "# Initialize generator\n",
    "\n",
    "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
    "\n",
    "# Generate some text\n",
    "\n",
    "settings = ExLlamaV2Sampler.Settings()\n",
    "settings.temperature = 0.85\n",
    "settings.top_k = 50\n",
    "settings.top_p = 0.8\n",
    "settings.token_repetition_penalty = 1.01\n",
    "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c469db96-4d2f-43d9-897a-bab0782867dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import textwrap\n",
    "\n",
    "def get_docstring(node):\n",
    "    if isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Str):\n",
    "        return node.body[0].value.s.strip()\n",
    "    return None\n",
    "\n",
    "def get_function_body(source_lines, start_lineno, end_lineno):\n",
    "    return \"\\n\".join(source_lines[start_lineno - 1:end_lineno])\n",
    "\n",
    "def parse_python_file(file_path):\n",
    "    functions = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        source_lines = file.readlines()\n",
    "        tree = ast.parse(\"\".join(source_lines), filename=file_path)\n",
    "\n",
    "    for node in tree.body:\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            # Get the start and end line numbers of the function definition\n",
    "            start_lineno = node.lineno\n",
    "            end_lineno = node.body[-1].lineno if node.body else node.lineno\n",
    "\n",
    "            # Grab any comments preceding the function definition\n",
    "            docstring = get_docstring(node)\n",
    "            if not docstring:\n",
    "                continue\n",
    "\n",
    "            # Get the function body including comments and docstrings\n",
    "            function_body = get_function_body(source_lines, start_lineno, end_lineno)\n",
    "\n",
    "            # Split docstring into description and examples\n",
    "            description, _, examples = docstring.partition(\"Examples\")\n",
    "\n",
    "            # Remove extra leading/trailing whitespaces\n",
    "            description = description.strip()\n",
    "\n",
    "            # Format examples nicely\n",
    "            examples = textwrap.dedent(examples.strip())\n",
    "\n",
    "            function_split = function_body.split('\"\"\"')\n",
    "            if len(function_split) > 2:\n",
    "                function_body = function_split[0] + function_split[2]\n",
    "            description_split = description.split(\"Examples\\n--------\")\n",
    "            description = description_split[0]\n",
    "\n",
    "            functions.append((node.name, function_body, description, examples))\n",
    "\n",
    "    return functions\n",
    "def get_specific_function(functions, desired_function):\n",
    "    for func_name, function_body, description, examples in functions:\n",
    "        if func_name == desired_function:\n",
    "            return func_name, function_body, description, examples\n",
    "    return \"Unable to Find\"\n",
    "def generate_prompt(function1, function2, function3, unknown_function):\n",
    "    prompt = \"Function: \" + function1[0] + \"\\nFunction Body: \" + function1[1] + \"\\nDocstrings: \"+ function1[2] + \"\\nExamples: \"+ function1[3]\n",
    "    prompt += \"\\nFunction: \" + function2[0] + \"\\nFunction Body: \" + function2[1] + \"\\nDocstrings: \"+ function2[2] + \"\\nExamples: \"+ function2[3]\n",
    "    prompt += \"\\nFunction: \" + function3[0] + \"\\nFunction Body: \" + function3[1] + \"\\nDocstrings: \"+ function3[2] + \"\\nExamples: \"+ function3[3]\n",
    "    prompt += \"\\nFunction: \" + unknown_function[0] + \"\\nFunction Body: \" + unknown_function[1] + \"\\nDocstrings: \"+ unknown_function[2] + \"\\nExamples: ------\"\n",
    "    return prompt    \n",
    "def write_to_file(filename, string):\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(\"Assistant\" + string + \"\\n\\n\")\n",
    "        file.write(\"-------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    print(\"finished\")\n",
    "\n",
    "def get_examples(functions):\n",
    "    file_path = \"_linalg.py\"\n",
    "    lin_functions = parse_python_file(file_path)\n",
    "    \n",
    "    function1 = get_specific_function(lin_functions, \"inv\")\n",
    "    function2 = get_specific_function(lin_functions, \"det\")\n",
    "    function3 = get_specific_function(lin_functions, \"eigvals\")\n",
    "\n",
    "    file_path = \"core.py\"\n",
    "    \n",
    "    core_functions = parse_python_file(file_path)\n",
    "    \n",
    "    function4 = get_specific_function(core_functions, \"minimum_fill_value\")\n",
    "    function5 = get_specific_function(core_functions, \"asarray\")\n",
    "    \n",
    "    for function in functions:\n",
    "        unknown_function = function\n",
    "        unknown_function_name = function[0]\n",
    "        prompt = generate_prompt(function1, function3, function5, unknown_function)\n",
    "    \n",
    "        system_prompt = \"You take in three example functions with sections Function, Function Body, Docstrings, and Examples. \"\n",
    "        system_prompt += \"You are given a fourth function, function body, and docstring. \" \n",
    "        system_prompt += f\"Please write examples for the {unknown_function_name} function. \" \n",
    "        system_prompt += f\"Provide only examples for the {unknown_function_name} function. \"\n",
    "        system_prompt += \"Do not provide repeat examples. \"\n",
    "        system_prompt += \"Do not start an example without finishing it. \"\n",
    "        \n",
    "        texts = [f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|>\\n\"]\n",
    "        texts.append(f'<|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>')\n",
    "        rendered_prompt = ''.join(texts)\n",
    "\n",
    "        rendered_prompt\n",
    "        tokenizer.encode(rendered_prompt)\n",
    "        tokenizer.encode(rendered_prompt).shape[-1]\n",
    "\n",
    "        max_new_tokens = 512\n",
    "\n",
    "        generator.warmup()\n",
    "        time_begin = time.time()\n",
    "\n",
    "        output = generator.generate_simple(rendered_prompt, settings, max_new_tokens, seed = 1234)\n",
    "\n",
    "        time_end = time.time()\n",
    "        time_total = time_end - time_begin\n",
    "\n",
    "        #print(output)\n",
    "        #print()\n",
    "        print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")\n",
    "        assistant_texts = output.split(\"assistant\")\n",
    "        write_to_file(\"examples_testing_70.txt\", assistant_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeeef83-c894-4b25-9bd5-82b11b6ee759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response generated in 165.27 seconds, 512 tokens, 3.10 tokens/second\n",
      "finished\n",
      "Response generated in 175.89 seconds, 512 tokens, 2.91 tokens/second\n",
      "finished\n",
      "Response generated in 183.31 seconds, 512 tokens, 2.79 tokens/second\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "file_path = \"_linalg.py\"\n",
    "functions = parse_python_file(file_path)\n",
    "list_of_functions = []\n",
    "list_of_functions.append(get_specific_function(functions, \"svdvals\"))\n",
    "list_of_functions.append(get_specific_function(functions, \"cond\"))\n",
    "list_of_functions.append(get_specific_function(functions, \"matrix_rank\"))\n",
    "list_of_functions.append(get_specific_function(functions, \"eigh\"))\n",
    "get_examples(list_of_functions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyst-analyst-basic_exl2",
   "language": "python",
   "name": "conda-env-analyst-analyst-basic_exl2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
